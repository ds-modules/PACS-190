{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis 03: Document-Term Matrix and Sentiment Analysis\n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"data/word_magnets.jpg\" style=\"width: 400px; height: 300px;\" />\n",
    " *Photo by [Steve Johnson](https://www.flickr.com/photos/artbystevejohnson/4654424717)* \n",
    "\n",
    "### Professor Crystal Chang\n",
    "\n",
    "This notebook will extend the word count approach from the last module to multiple documents with Document Term Matrices. We will also introduce sentiment analysis.\n",
    "\n",
    "*Estimated Time: 60 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "[The Data](#section data)<br>\n",
    "[Context](#section context)<br>\n",
    "1 - [Working with Multiple Documents](#section 1)<br>\n",
    "2 - [The Term-Document Matrix](#section 2)<br>\n",
    "3 - [Sentiment Analysis](#section 3)<br>\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "!pip install textblob\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "One of the powerful things about text analysis with Python is the ability to work with a large number of documents simultaneously. In this notebook, we'll expand Term Frequency analysis to cover multiple text documents in a **Term Document Matrix**. \n",
    "\n",
    "The term-document model is also sometimes referred to as \"bag-of-words\" by those who don't think very highly of it. The term document model looks at language as individual communicative efforts that contain one or more tokens. The kind and number of the tokens in a document tells you something about what is attempting to be communicated, and the order of those tokens is ignored. This is the primary method still used for most text analysis.\n",
    "\n",
    "We'll also learn another method of text analysis by **Sentiment**. We'll look at the tools to conduct sentiment analysis, and explore pros and cons to such an approach.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. Working with Multiple Documents <a id='section 1'></a>\n",
    "\n",
    "In order to actually turn our text into a bag of words, we'll have to do some preprocessing. This is a crucial step at the beginning of any NLP project, and much of this first section will involve it.\n",
    "\n",
    "To start with, let's import NLTK and load some data: tweets from @realDonaldTrump. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the data from a file\n",
    "trump = pd.read_csv('data/trumptweets.csv', header=0, index_col=0)\n",
    "\n",
    "# display the table of data\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same dataframe generated from the demo! Data isn't always pretty, but there are ways to process and clean it up.\n",
    "\n",
    "For now, let's just focus on the `text` column. We can still accomplish a lot by extracting the text of the tweets as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(trump['text'])\n",
    "tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "Just as in the last module, when we worked with a single text document, we also want to do some **pre-processing** on this text to do better analysis. Here, the text analysis tools we're using are more sophisticated: they can automatically do things like lower the case of text and remove stop words! But, we do have some noise in our data we want to remove. \n",
    "\n",
    "Take another look at two tweets in particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets[4])\n",
    "print()\n",
    "print(tweets[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the fifth tweet (and many tweets, in fact) contain a link, which probably won't tell us much about Trump's word usage. We can also see that some tweets contain characters that aren't from the English alphabet. It would be helpful to filter these out before we look at things like word frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions \n",
    "\n",
    "### Overview\n",
    "\n",
    "Regular expressions (regex or regexp for short) are special sequences of characters that define patterns\n",
    "to search for in text. They're often used in find-and-replace operations, or to add up the number of words\n",
    "or phrases matching a particular pattern.\n",
    "\n",
    "Regular expressions are useful in a variety of applications, and can be used in different programs and\n",
    "programming languages. We will start by learning the general components of regular expressions, using a\n",
    "simple online tool, RegExr. We'll also demonstrate how to use them in Python.\n",
    "\n",
    "To get started:\n",
    "\n",
    "1. Go to this site: [http://regexr.com](http://regexr.com).\n",
    "2. Copy and paste the two tweets we just printed into the __Text__ field.\n",
    "3. Delete what you see in the __Expression__ field. This is where we'll insert our own regular expressions\n",
    "to find sequences in the headlines below.\n",
    "\n",
    "~~~ {.input}\n",
    "We thank you. We salute you. We honor you. And we promise you: we will ALWAYS have your BACK â€“ now and FOREVER! #IACP2018 https://t.co/nvUUIuvouj\n",
    "\n",
    "RT @FLOTUS: Thank you Kenya ðŸ‡°ðŸ‡ª ðŸ‡ºðŸ‡¸ https://t.co/JrHncob8Qp\n",
    " \n",
    "~~~\n",
    "\n",
    "### a. Special Characters\n",
    "\n",
    "Strings are composed of characters, and we are writing patterns to match specific sequences of characters.\n",
    "Various characters have special meaning in regular expressions. When we use these characters in an expression,\n",
    "we aren't matching the identical character, we're using the character as a placeholder for some other character(s)\n",
    "or part(s) of a string.\n",
    "\n",
    "If you want to match a character that happens to be a special character, you have to escape it with a backslash\n",
    "`\\`. Try typing the following special characters into the __Expression__ field on the regexr.com site. What happens\n",
    "when you type `Prime Minister Abe` vs. `^Prime Minister Abe`? How about `.`, `\\.`, or `\\.$`?\n",
    "\n",
    "~~~ {.input}\n",
    ".         any single character\n",
    "^         start of string\n",
    "$         end of string\n",
    "\\n        new line\n",
    "\\r        carriage return\n",
    "\\t        tab\n",
    "~~~\n",
    "\n",
    "### b. Quantifiers\n",
    "\n",
    "Some special characters refer to optional characters, to a specific number of characters, or to an open-ended\n",
    "number of characters matching the preceding pattern. Try looking for the letter 'e' followed by a number of 's's:\n",
    "what happens if you type `es`, `es*`, `es+`, `es{1}`, `es{1,2}`?\n",
    "\n",
    "~~~ {.input}\n",
    "*        0 or more of the preceding character/expression\n",
    "+        1 or more of the preceding character/expression\n",
    "?        0 or 1 of the preceding character/expression\n",
    "{n}      n copies of the preceding character/expression \n",
    "{n,m}    n to m copies of the preceding character/expression \n",
    "~~~\n",
    "\n",
    "### c. Sets\n",
    "\n",
    "Regular expressions also allow you to define sets of characters. Within a set of square brackets, you may list\n",
    "characters individually, e.g. `[aeiou]`, or in a range, e.g. `[A-Z]` (note that all regular expressions are case\n",
    "sensitive).\n",
    "\n",
    "You can also create a complement set by excluding certain characters, using `^` as the first character\n",
    "in the set. The set `[^A-Za-z]` will match any character except a letter. All other special characters loose\n",
    "their special meaning inside a set, so the set `[.?]` will look for a literal period or question mark.\n",
    "\n",
    "The set will match only one character contained within that set, so to find sequences of multiple characters from\n",
    "the same set, use a quantifier like `+` or a specific number or number range `{n,m}`.\n",
    "\n",
    "~~~ {.input}\n",
    "[0-9]        any numeric character\n",
    "[a-z]        any lowercase alphabetic character\n",
    "[A-Z]        any uppercase alphabetic character\n",
    "[aeiou]      any vowel (i.e. any character within the brackets)\n",
    "[0-9a-z]     to combine sets, list them one after another \n",
    "[^...]       exclude specific characters\n",
    "~~~\n",
    "\n",
    "### d. Special sequences\n",
    "\n",
    "Several special characters denote special sequences. These begin with a `\\` followed by a letter.\n",
    "Note that the uppercase version is usually the complement of the lowercase version.\n",
    "\n",
    "~~~ {.input}\n",
    "\\d        Any digit\n",
    "\\D        Any non-digit character\n",
    "\\w        Any alphanumeric character [0-9a-zA-Z_] \n",
    "\\W        Any non-alphanumeric character\n",
    "\\s        Any whitespace (space, tab, new line)\n",
    "\\S        Any non-whitespace character\n",
    "\\b        Matches the beginning or end of a word (does not consume a character)\n",
    "\\B        Matches only when the position is not the beginning or end of a word (does not consume a character)\n",
    "~~~\n",
    "\n",
    "### e. Groups and Logical OR\n",
    "\n",
    "Parentheses are used to designate groups of characters, to aid in logical conditions, and to be able to retrieve the\n",
    "contents of certain groups separately.\n",
    "\n",
    "The pipe character `|` serves as a logical OR operator, to match the expression before or after the pipe. Group parentheses\n",
    "can be used to indicate which elements of the expression are being operated on by the `|`.\n",
    "\n",
    "~~~ {.input}\n",
    "|            Logical OR opeator\n",
    "(...)        Matches whatever regular expression is inside the parentheses, and notes the start and end of a group\n",
    "(this|that)  Matches the expression \"this\" or the expression \"that\"\n",
    "~~~\n",
    "\n",
    "## regex in Python\n",
    "\n",
    "Important methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.findall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Find a regex pattern that will match a url. Then, remove the url from `url_tweet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url_tweet = tweets[4]\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Ultimately, we want to remove urls and non-English characters from *all* tweets in `tweets`. Instead of going through each tweet individually, let's use another **list comprehension**.\n",
    "\n",
    "Remember, the syntax is:\n",
    "\n",
    "`[<do_something(item)> for <item> in <sequence> if <condition>]`\n",
    "\n",
    "In this case, though, there's no condition: we want to change every tweet. So, our code will look like this:\n",
    "\n",
    "`[<do_something(item)> for <item> in <sequence>]`\n",
    "\n",
    "Replace the ellipses with expressions to remove the urls and non-English characters from the tweets. \n",
    "- The first line creates a new list called `no_urls` by removing the urls from each tweet in `tweets`\n",
    "- The second line creates a new list called  `no_urls_all_engl` by removing all non-English characters from the tweets in `no_urls`. Hint: the regex for matching non-English characters is `'[^\\x00-\\x7F]+'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_urls = [... for x in tweets] \n",
    "no_urls_all_engl = [... for x in no_urls] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_urls_all_engl[4])\n",
    "print()\n",
    "print(no_urls_all_engl[9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to `text` column\n",
    "\n",
    "Now that we have a list comprehension to remove URLs, let's apply this to the `text` column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump['text'] = [... for x in trump['text']] #Your code here\n",
    "\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **More regex**\n",
    "\n",
    "Take a look at one of the values in the `source` column. \n",
    " `<a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>`\n",
    " \n",
    "Let's use regex to clean this up and only include the text between the tags.\n",
    " \n",
    "**Use list comprehension to clean the source**\n",
    " \n",
    "hint: The regex for matching the string between the tags is `^[^>]*>\"`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump['source'] = [... for x in trump['source']]\n",
    "\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Term Document Matrix <a id='section 2'></a>\n",
    "\n",
    "If we plan to compare word frequencies across texts, we could collate these `Counter` dictionaries for each tweet in `tweets`. But we don't want to write all that code! There is an easy function that streamlines the process called `CountVectorizer`.\n",
    "\n",
    "Let's look at the docstring:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CountVectorizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool. So we'll create the `CountVectorizer` object, then transform it on our `list` of documents: the tweets in  `no_urls_all_engl`. We can give `CountVectorizer` a list of stop words to automatically exclude them from our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "stop_words = ENGLISH_STOP_WORDS\n",
    "\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "dtm = cv.fit_transform(no_urls_all_engl)\n",
    "dtm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's this? A sparse matrix just means that some cells in the table don't have value. Why? Because the vocabulary base is not the same for all the books! Let's try to demonstrate this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# de-sparsify\n",
    "desparse = dtm.toarray()\n",
    "\n",
    "# create labels for columns\n",
    "word_list = cv.get_feature_names()\n",
    "\n",
    "# create a new table\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=desparse)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the ***Document Term Matrix***. This is a core concept in NLP and text analysis. It's not that complicated!\n",
    "\n",
    "We have columns for each word *in the entire corpus*. Then each *row* is for each *document*. In our case, that's tweets. The values are the word count for that word in the corresponding document. Note that there are many 0s, that word just doesn't show up in that document!\n",
    "\n",
    "We can call up frequencies for a given word for each tweet easily, since they are the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df['news'][3225:3235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can see the total usage of a given word using `sum`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df['news'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "Let's take this another step further. In order to make apples-to-apples comparisons across tweets, we can normalize our values by dividing each word count by the total number of words in its tweet. To do that, we'll need to `sum` on `axis=1`, which means summing the row (number of words in that tweet), as opposed to summing the column.\n",
    "\n",
    "Once we have the total number of words in that tweet, we can get the percentage of words that one particular word accounts for, and we can do that for every word across the matrix!\n",
    "\n",
    "Note: normalization is not a huge deal for tweets, since each tweet is limited to 280 characters. But, if you're comparing documents with very different lengths, normalization is key to getting an accurate picture of word usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row_sums = np.sum(desparse, axis=1)\n",
    "normed = desparse/row_sums[:,None]\n",
    "dtm_df = pd.DataFrame(columns=word_list, data=normed)\n",
    "dtm_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can still get the normalized frequencies of the word 'news' for each tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_df['news'][3225:3235]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Streamlining\n",
    "\n",
    "That was a lot of work; if this is such a common task hasn't someone streamlined this? In fact, we can simply instruct `CountVectorizer` not to include stopwords at all and another function, `TfidfTransformer`, normalizes easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "dtm = cv.fit_transform(no_urls_all_engl)\n",
    "tt = TfidfTransformer(norm='l1',use_idf=False)\n",
    "dtm_tf = tt.fit_transform(dtm)\n",
    "dtm_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sentiment <a id='section 3'></a>\n",
    "\n",
    "Frequently, we are interested in text to learn something about the person who is speaking. One of these things we've talked about already - linguistic diversity. A similar metric was used a couple of years ago to settle the question of who has the [largest vocabulary in Hip Hop](http://poly-graph.co/vocabulary.html).\n",
    "\n",
    "> Unsurprisingly, top spots go to Canibus, Aesop Rock, and the Wu Tang Clan. E-40 is also in the top 20, but mostly because he makes up a lot of words; as are OutKast, who print their lyrics with words slurred in the actual typography\n",
    "\n",
    "Another thing we can learn is about how the speaker is feeling, with a process called sentiment analysis. Before we start, be forewarned that this is not a robust method by any stretch of the imagination. Sentiment classifiers are often trained on product reviews, which limits their ecological validity.\n",
    "\n",
    "We're going to use TextBlob because it's an easy way to work with text data, and has a built in sentiment classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = TextBlob(no_urls_all_engl[11])\n",
    "blob.sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the polarity of a sentence, we can just use `.polarity`. Polarity is a number between -1 and 1, where -1 is considered 'negative', 0 is 'neutral' and 1 is 'positive'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentences[1].polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to get the polarity of a tweet, we can use `.polarity` on the blob itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening behind the scenes? While there are new algorithms for sentiment anaysis emerging (cf. `VADER`), most algorithms currently rely only on a `dictionary` of words and a corresponding `positive`, `negative`, or `neutral`. Based on all the words in a sentence, a value is calculated for the sentence as a whole. And, polarity for a tweet is calculated as the average polarity of all sentences in the tweet. Not super fancy, I know. Of course, you can change the `dictionary` used in the library itself, or opt for more advanced algorithms that aim to capture context.\n",
    "\n",
    "We can also get the **subjectivity** of a tweet. Subjectivity ranges from 0 ('objective') to 1 ('subjective') and is related to the different possible meanings a word can take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.subjectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can get both at once using `.sentiment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Write a list comprehension to calculate the sentiment of all tweets in `no_url_all_engl`. \n",
    "\n",
    "Hint: it may be easier to write two list comprehensions- one to convert each tweet into a TextBlob, and one to calculate the sentiment for each blob in your TextBlob list. However, you can do it in a single list comprehension!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here - make sure to name the final result as \"sentiments\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's add the net polarities to our original table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump['polarity'] = [s[0] for s in sentiments]\n",
    "trump['subjectivity'] = [s[1] for s in sentiments]\n",
    "\n",
    "trump.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Further Resources <a id='section 4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These modules have covered just a few of the text analysis methods out there today. If you'd like to learn more, or if you're interested in using these techniques in your own text analysis project, here's some resources you might find helpful.\n",
    "\n",
    "- Located on the first floor of Moffitt Library, the [Data + Digital Research Help](https://data.berkeley.edu/education/data-digital-research-help) service provide students a hub for support in their co-curricular data science projects. \n",
    "\n",
    "- [D-Lab](http://dlab.berkeley.edu/calendar-node-field-date) provides a variety of free workshop trainings for those interested in learning Python, R, Stata, Excel, Geospatial Mapping, Qualitative Methods, etc.\n",
    "\n",
    "- The [Data Lab](http://www.lib.berkeley.edu/libraries/data-lab) offers consultations to current UC Berkeley students, staff and faculty on research involving numeric data, including finding and recommending data sources and advising on technical data issues such as file format conversion, web scraping, and basic data analysis assistance. \n",
    "- D-Lab also hosts a robust list of [campus data-related resources](http://dlab.berkeley.edu/dlab-campus-resources), including places to get data set and data analysis support\n",
    "\n",
    "\n",
    "If you're up for a challenge, you can look at more text analysis examples\n",
    "- https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk  \n",
    "Datacamp is also a great resource to learn Data Science online. \n",
    "\n",
    "- More indepth Bag of Words\n",
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "\n",
    "- Tokenizing words w/ NLTK https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\n",
    "\n",
    "There are tons of resources online!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### [Feedback survey](https://docs.google.com/forms/d/1qk4za1wvVSvug4yHvUtXFMTJHhbkIUjxOl8TwMMCDl4/edit?ts=5a8de458)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Regex section taken from materials by Chris Hench. https://github.com/henchc/textxd-2017/blob/master/03-regex.ipynb\n",
    "- Document Term Matrix section adapted from materials by Chris Hench: https://github.com/henchc/textxd-2017/blob/master/06-DTM.ipynb\n",
    "- Sentiment section adapted from the D-Lab's \"Intro to Text Analysis\" workshop: https://github.com/dlab-berkeley/python-text-analysis/blob/master/Intro_to_TextAnalysis/Intro_to_TextAnalysis.ipynb\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto  \n",
    "Modified by: Tina Nguyen\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
