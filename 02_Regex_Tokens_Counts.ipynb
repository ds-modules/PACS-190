{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis 02: Pre-Processing and Word Frequency\n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/2a/ELI-LA-Word-Cloud-2-900x600.jpg\" style=\"width: 500px; height: 275px;\" />\n",
    "\n",
    "### Professor Crystal Chang\n",
    "\n",
    "This notebook will cover tools and strategies used in data pre-processing to prepare text data for analysis. We will then look at how to get basic word counts.\n",
    "\n",
    "*Estimated Time: 45 minutes*\n",
    "\n",
    "---\n",
    "\n",
    "### Topics Covered\n",
    "- Text Pre-processing\n",
    "- Word Frequency\n",
    "\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "\n",
    "1 - [Tokens](#section 1)<br>\n",
    "\n",
    "2 - [Counting Tokens](#section 2)<br>\n",
    "\n",
    "3 - [Stop Words](#section 3)<br>\n",
    "\n",
    "4 - [N-grams](#section 4)<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We've spent a lot of time in Python dealing with strings, and that's because strings can represent text data, and text data is everywhere. It is the primary form of communication between persons and persons, persons and computers, and computers and computers. \n",
    "\n",
    "This notebook will apply the methods you just learned to a fundamental component of text analysis: **pre-processing**, or getting the text into a form Python can understand and manipulate. We will then use our processed text to calculate ** word frequency**, the foundation of the Bag-of-Words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokens <a id='section 1'></a>\n",
    "\n",
    "What is a token? In text analysis, we need something to count in order to quantify language. For NLP practitioners, we count tokens. A token is often simply a word. But more accurately, a token is the unit of analysis for an NLP project. This may be a word, but it may be a stem, or lemma too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "Let's first read in an example text to work with. In this section, we'll use the text from President Obama's final speech at the United Nations General Assembly on September 28, 2015:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/obama_un_2015.txt') as f:\n",
    "    obama_un = f.read()\n",
    "\n",
    "print(obama_un)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may be tempted to use our `str` knowledge and just call `.split()` to get our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = obama_un.split()\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good. What if we wanted to get some counts? Let's try something we know is in the document: the very first word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.count('Mr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what? We can see two 'Mr's in the first sentence alone! Let's take another look at our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that periods stayed with our token \"Mr\". The `.split()` method split on white-space only. It won't intuitively know what a word is.\n",
    "\n",
    "Similarly, Python won't know the difference between \"It\" and \"it\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.count('It')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens.count('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick and dirty way to get some normalized text back would be to remove punctuation and lower-case the entire string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc = ''.join([c for c in obama_un if c not in punctuation])\n",
    "no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_lower = no_punc.lower()\n",
    "no_punc_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_lower_tokens = no_punc_lower.split()\n",
    "no_punc_lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_lower_tokens.count('mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punc_lower_tokens.count('It'), no_punc_lower_tokens.count('it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Write a function that tokenizes your text. Your function should do three things:\n",
    "- remove the punctuation\n",
    "- lower-case the text\n",
    "- split the text on white spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    no_punc = ... #your code here\n",
    "    no_punc_lower = ... #your code here\n",
    "    no_punc_lower_tokens = ... #your code here\n",
    "    return no_punc_lower_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your function on a snippet of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snippet = obama_un[25195:]\n",
    "print(snippet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join([c for c in 'tomorrowâ€™s' if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting Tokens <a id='section 2'></a>\n",
    "\n",
    "The easiest way to count tokens is using the `Counter` object from `collections`. This will give you back a dictionary with the token counts. Take a look at the docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try it out on `no_punc_lower_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(no_punc_lower_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `.most_common()` method will sort tokens by how frequent they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(no_punc_lower_tokens).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Stop words <a id='section 3'></a>\n",
    "\n",
    "You've probably noticed that a lot of our most frequent words, bigrams, and trigrams aren't very interesting: they contain a lot of **stop words** like 'and', 'the', and 'of'. Frequently, we'll want to remove the stop words in order to get more meaningful results from our analysis.\n",
    "\n",
    "Fortunately, the scikit-learn package has a list of English stop words we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Comprehensions\n",
    "When we're dealing with a list of items, we often want to check or alter each item. Instead of indexing each item individually, it's better to use a **list comprehension**. A list comprehension takes a list, goes through each item to change or remove it, and returns a new list.\n",
    "\n",
    "A basic list comprehension is written in the following order:\n",
    "\n",
    "`[<do_something(item)> for <item> in <sequence> if <condition>]`\n",
    "\n",
    "For example, to double the numbers in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1, 2, 3, 4, 5]\n",
    "\n",
    "[x * 2 for x in numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today, we're going to use list comprehensions to filter items. For example, we used one earlier to filter out punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = ['3', 'a', '.', '!', 'g']\n",
    "\n",
    "[c for c in characters if c not in punctuation]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list comprehension says \"Keep each item `c` in `characters` exactly as it is unless it is in the list of  `punctuation`\".\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Write a list comprehension that looks at each token in `tokens` and takes the token out if it is in the list of `ENGLISH_STOP_WORDS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(obama_un)\n",
    "\n",
    "tokens_no_stops = ... #Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens_no_stops).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. N-Grams <a id='section 4'></a>\n",
    "\n",
    "N-grams help us move beyond looking at one word, and allow us to look at co-occurence. It takes window snapshots of a list of words going one word at a time. It's easiest to understand from an example. First we'll define a function find_ngrams, don't worry about the code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(input_list, n):\n",
    "    return zip(*[input_list[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a bigram (window of 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams('I like walking my dog'.split(), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at bigrams for our tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(ngrams(no_punc_lower_tokens, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Counter` for this too!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(list(ngrams(no_punc_lower_tokens, 2))).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Write the code to get a list of trigrams (windows of 3) for `no_punc_lower_tokens`. Then, use `Counter` to find the most common trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "Should we remove stop words before looking at bi- and tri-grams? Why or why not?\n",
    "\n",
    "What kinds of research could we do using word or N-gram frequency analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokens and Preprocessing section adapted from materials by Chris Hench. https://github.com/henchc/textxd-2017/blob/master/04-Tokens-and-Pre-Processing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Notebook developed by: Keeley Takimoto\n",
    "\n",
    "Data Science Modules: http://data.berkeley.edu/education/modules\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
